---
title: "Feed me back now:"
subtitle: "Tales of automated feedback <br> in R and Python"
author: "Mine Çetinkaya-Rundel & Tiffany Timbers"
date: "2020/05/20 (updated: `r Sys.Date()`) <br> TODO: INSERT LINK TO SLIDES"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
# color inspiration: pantone 2020 colors
# https://store.pantone.com/uk/en/color-of-the-year-2020-palette-exploration
style_duo_accent(
  primary_color = "#0F4C81",   # pantone classic blue
  secondary_color = "#DBCCBE", # pantone pink tint
  black_color = "#7A8387",     # pantone monument
  text_color = "#2A2A35",      # pantone night sky
  header_font_google = google_font("Arvo"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Source Code Pro")
)
```

```{r load-packages, include=FALSE}
library(tidyverse)
```

## Outline

[TODO: WE CAN REMOVE THIS SLIDE, ADDING FOR NOW TO KEEP TRACK]

- Automated feedback: what, why, and how?
- Automated feedback in R:
  - **learnr** tutorials, feedback with **gradethis**
  - R Markdown documents, feedback with GitHub Actions
- Automated feedback in Python
  - ???
- Closing thoughts
  - Best practices
  - Getting started
  - Resources

---

class: middle, inverse

# Automated feedback: what, why, and how?

---

class: center, middle

## Why automated feedback?

---

## Code checking

```{r error = TRUE}
x + 2
```

---

class: center, middle

# Automated feedback in learnr

(Mine Çetinkaya-Rundel)

---

class: center, middle

# Automated feedback via GitHub Actions

(Mine Çetinkaya-Rundel)

---

class: center, middle

# Automated feedback in Jupyter
(Tiffany Timbers)

---

## What is Jupyter?

---

## Tests as automated feedback

---

## Autograding via nbgrader

---

class: center, middle

# Demo!

---

class: center, middle

# Closing thoughts

---

## Best practices for automated feedback

- Measure twice, cut once (verify the correctness of your tests).

- Use rounding & type coercion to write robust tests.

- Use hashing to hide solutions for visible tests.

- Test your tests on the students compute environment.

- Abstract your tests to a script that is sourced/imported into the students literate code document.

- Don't give automated feedback on everything.

---

## Resources

- [nbgrader docs](https://nbgrader.readthedocs.io/en/stable/) (autograding in Jupyter notebooks)
